{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Laten Semantic Analysis\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a method of text analysis \n",
    "that helps to identify concepts represented in the text as related words\n",
    "In this example, we both calculate document representation in the \n",
    "concept space and score documents againt the query using a distance metric in this space.\n",
    "\n",
    "| Description | See [Introduction to Algorithmic Marketing](https://algorithmicweb.wordpress.com/ ) book |\n",
    "|--|:--|\n",
    "| Dataset | Synthetic |\n",
    "| Libs | Sympy, Numpy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sympy as sy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain \n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "\n",
    "def tabprint(msg, A):\n",
    "    print(msg)\n",
    "    print(tabulate(A, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"chicago chocolate retro candies made with love\",\n",
    "\"chocolate sweets and candies collection with mini love hearts\",\n",
    "\"retro sweets from chicago for chocolate lovers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Basic analyzer: \n",
    "# - split documents into words\n",
    "# - remove stop words\n",
    "# - apply a simple stemmer\n",
    "analyzer = {\n",
    "    \"with\": None,\n",
    "    \"for\": None,\n",
    "    \"and\": None,\n",
    "    \"from\": None,\n",
    "    \"lovers\": \"love\",\n",
    "    \"hearts\": \"heart\"\n",
    "}\n",
    "bag_of_words_docs = [list(filter(None, [analyzer.get(word, word) for word in d.split()])) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   candies [1 1 0]\n",
      "collection [0 1 0]\n",
      " chocolate [1 1 1]\n",
      "     heart [0 1 0]\n",
      "   chicago [1 0 1]\n",
      "    sweets [0 1 1]\n",
      "      made [1 0 0]\n",
      "     retro [1 0 1]\n",
      "      love [1 1 1]\n",
      "      mini [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Create term frequency matrix\n",
    "unique_words = list(set(chain.from_iterable(bag_of_words_docs)))\n",
    "word_freq = [Counter(d) for d in bag_of_words_docs]\n",
    "A = np.array([[freq.get(word, 0) for freq in word_freq] for word in unique_words])\n",
    "for i, word in enumerate(unique_words):\n",
    "    print(\"%10s %s\" % (word, str(A[i])))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ut =\n",
      "╒═══════════╤════════════╕\n",
      "│ -0.333888 │ -0.148947  │\n",
      "├───────────┼────────────┤\n",
      "│ -0.167762 │ -0.4059    │\n",
      "├───────────┼────────────┤\n",
      "│ -0.485799 │  0.0183087 │\n",
      "├───────────┼────────────┤\n",
      "│ -0.167762 │ -0.4059    │\n",
      "├───────────┼────────────┤\n",
      "│ -0.318037 │  0.424208  │\n",
      "├───────────┼────────────┤\n",
      "│ -0.319674 │ -0.238644  │\n",
      "├───────────┼────────────┤\n",
      "│ -0.166126 │  0.256953  │\n",
      "├───────────┼────────────┤\n",
      "│ -0.318037 │  0.424208  │\n",
      "├───────────┼────────────┤\n",
      "│ -0.485799 │  0.0183087 │\n",
      "├───────────┼────────────┤\n",
      "│ -0.167762 │ -0.4059    │\n",
      "╘═══════════╧════════════╛\n",
      "St =\n",
      "╒═════════╤═════════╕\n",
      "│ 3.56192 │ 0       │\n",
      "├─────────┼─────────┤\n",
      "│ 0       │ 1.96588 │\n",
      "╘═════════╧═════════╛\n",
      "Vt =\n",
      "╒═══════════╤═══════════╤═══════════╕\n",
      "│ -0.591727 │ -0.597555 │ -0.541097 │\n",
      "├───────────┼───────────┼───────────┤\n",
      "│  0.505138 │ -0.79795  │  0.328805 │\n",
      "╘═══════════╧═══════════╧═══════════╛\n",
      "Ut x St x Vt =\n",
      "╒════╤════╤═══╕\n",
      "│  1 │  1 │ 1 │\n",
      "├────┼────┼───┤\n",
      "│ -0 │  1 │ 0 │\n",
      "├────┼────┼───┤\n",
      "│  1 │  1 │ 1 │\n",
      "├────┼────┼───┤\n",
      "│ -0 │  1 │ 0 │\n",
      "├────┼────┼───┤\n",
      "│  1 │  0 │ 1 │\n",
      "├────┼────┼───┤\n",
      "│  0 │  1 │ 0 │\n",
      "├────┼────┼───┤\n",
      "│  1 │ -0 │ 0 │\n",
      "├────┼────┼───┤\n",
      "│  1 │  0 │ 1 │\n",
      "├────┼────┼───┤\n",
      "│  1 │  1 │ 1 │\n",
      "├────┼────┼───┤\n",
      "│ -0 │  1 │ 0 │\n",
      "╘════╧════╧═══╛\n"
     ]
    }
   ],
   "source": [
    "# Perform truncated SVD decomposition \n",
    "U, s, V = np.linalg.svd(A, full_matrices=False)\n",
    "truncate_rank = 2\n",
    "Ut = U[:, 0:truncate_rank]\n",
    "Vt = V[0:truncate_rank, :]\n",
    "St = np.diag(s[0:truncate_rank])\n",
    "reconstruction = np.dot(Ut, np.dot(St, Vt))\n",
    "tabprint(\"Ut =\", Ut)\n",
    "tabprint(\"St =\", St)\n",
    "tabprint(\"Vt =\", Vt)\n",
    "tabprint(\"Ut x St x Vt =\", np.round(reconstruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 score: 0.890730150933\n",
      "Document 1 score: -0.51043666768\n",
      "Document 2 score: 0.806592806364\n"
     ]
    }
   ],
   "source": [
    "# Project a query to the concept space and score documents\n",
    "query = \"chicago\"\n",
    "q = [int(query == word) for word in unique_words]\n",
    "qs = np.dot(q, np.dot(Ut, np.linalg.inv(St)))\n",
    "\n",
    "def score(query_vec, doc_vec):\n",
    "    return np.dot(query_vec, doc_vec) / ( np.linalg.norm(query_vec) * np.linalg.norm(doc_vec) )\n",
    "\n",
    "for d in range(len(docs)):\n",
    "    print(\"Document %s score: %s\" % (d, score(qs, Vt[:, d])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
